{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from data_sci.fastai.nlp import *\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDB dataset and the sentiment classification task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [large movie review dataset](http://ai.stanford.edu/~amaas/data/sentiment/) contains a collection of 50,000 reviews from IMDB. The dataset contains an even number of positive and negative reviews. The authors considered only highly polarized reviews. A negative review has a score ≤ 4 out of 10, and a positive review has a score ≥ 7 out of 10. Neutral reviews are not included in the dataset. The dataset is divided into training and test sets. The training set is the same 25,000 labeled reviews.\n",
    "\n",
    "The **sentiment classification task** consists of predicting the polarity (positive or negative) of a given text.\n",
    "\n",
    "To get the dataset, in your terminal run the following commands:\n",
    "\n",
    "`wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz`\n",
    "\n",
    "`gunzip aclImdb_v1.tar.gz`\n",
    "\n",
    "`tar -xvf aclImdb_v1.tar`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Tokenizing and term document matrix creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH='data/aclImdb/'\n",
    "PATH='/data/msnow/data_science/imdb/aclImdb/'\n",
    "names = ['neg','pos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imdbEr.txt  imdb.vocab  README  \u001b[0m\u001b[01;34mtest\u001b[0m/  \u001b[01;34mtrain\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "%ls {PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labeledBow.feat  \u001b[0m\u001b[01;34mpos\u001b[0m/    unsupBow.feat  urls_pos.txt\r\n",
      "\u001b[01;34mneg\u001b[0m/             \u001b[01;34munsup\u001b[0m/  urls_neg.txt   urls_unsup.txt\r\n"
     ]
    }
   ],
   "source": [
    "%ls {PATH}train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0_9.txt\n",
      "10000_8.txt\n",
      "10001_10.txt\n",
      "10002_7.txt\n",
      "10003_8.txt\n",
      "10004_8.txt\n",
      "10005_7.txt\n",
      "10006_7.txt\n",
      "10007_7.txt\n",
      "10008_7.txt\n",
      "ls: write error\n"
     ]
    }
   ],
   "source": [
    "%ls {PATH}train/pos | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "??texts_labels_from_folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn,trn_y = texts_labels_from_folders(f'{PATH}train',names)\n",
    "val,val_y = texts_labels_from_folders(f'{PATH}test',names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the text of the first review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[`CountVectorizer`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) converts a collection of text documents to a matrix of token counts (part of `sklearn.feature_extraction.text`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "veczr = CountVectorizer(tokenizer=tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`fit_transform(trn)` finds the vocabulary in the training set. It also transforms the training set into a term-document matrix. Since we have to apply the *same transformation* to your validation set, the second line uses just the method `transform(val)`. `trn_term_doc` and `val_term_doc` are sparse matrices. `trn_term_doc[i]` represents training document i and it contains a count of words for each document for each word in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_term_doc = veczr.fit_transform(trn)\n",
    "val_term_doc = veczr.transform(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<25000x75132 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 3749745 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_term_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x75132 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 189 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_term_doc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aussie', 'aussies', 'austen', 'austeniana', 'austens']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = veczr.get_feature_names(); vocab[5000:5005]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\"controversial\"',\n",
       " '(no',\n",
       " '/><br',\n",
       " '/>i',\n",
       " '/>the',\n",
       " '/>what',\n",
       " '1967.',\n",
       " '40',\n",
       " 'a',\n",
       " 'about',\n",
       " 'ago,',\n",
       " 'all',\n",
       " 'also',\n",
       " 'am',\n",
       " 'america.',\n",
       " 'and',\n",
       " 'answer',\n",
       " 'any',\n",
       " 'anyone',\n",
       " 'are',\n",
       " 'arguably',\n",
       " 'around',\n",
       " 'artistic',\n",
       " 'as',\n",
       " 'asking',\n",
       " 'at',\n",
       " 'attentions',\n",
       " 'average',\n",
       " 'be',\n",
       " 'because',\n",
       " 'being',\n",
       " 'bergman,',\n",
       " 'between',\n",
       " 'between,',\n",
       " 'boy',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'centered',\n",
       " 'certain',\n",
       " 'cheaply',\n",
       " 'cinema.',\n",
       " 'classmates,',\n",
       " 'commend',\n",
       " 'considered',\n",
       " 'controversy',\n",
       " 'country,',\n",
       " 'countrymen',\n",
       " 'curious-yellow',\n",
       " 'customs',\n",
       " 'denizens',\n",
       " 'do',\n",
       " 'documentary',\n",
       " \"doesn't\",\n",
       " 'drama',\n",
       " 'enter',\n",
       " 'even',\n",
       " 'ever',\n",
       " 'everything',\n",
       " 'fact',\n",
       " 'fan',\n",
       " 'far',\n",
       " 'few',\n",
       " 'film',\n",
       " 'filmmakers',\n",
       " 'films',\n",
       " 'films.<br',\n",
       " 'find',\n",
       " 'first',\n",
       " 'focus',\n",
       " 'for',\n",
       " 'ford,',\n",
       " 'from',\n",
       " 'good',\n",
       " 'had',\n",
       " 'has',\n",
       " 'have',\n",
       " 'heard',\n",
       " 'her',\n",
       " 'his',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'ingmar',\n",
       " 'intended)',\n",
       " 'is',\n",
       " 'issues',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'john',\n",
       " 'just',\n",
       " 'kills',\n",
       " 'learn',\n",
       " 'lena',\n",
       " 'life.',\n",
       " 'like',\n",
       " 'made',\n",
       " 'major',\n",
       " 'make',\n",
       " 'making',\n",
       " 'married',\n",
       " 'me',\n",
       " 'meat',\n",
       " 'men.<br',\n",
       " 'mind',\n",
       " 'money',\n",
       " 'much',\n",
       " 'my',\n",
       " 'myself.<br',\n",
       " 'named',\n",
       " 'not',\n",
       " 'nudity',\n",
       " 'of',\n",
       " 'old',\n",
       " 'on',\n",
       " 'opinions',\n",
       " 'ordinary',\n",
       " 'particular',\n",
       " 'people',\n",
       " 'plot',\n",
       " 'plot.',\n",
       " 'political',\n",
       " 'politicians',\n",
       " 'politics,',\n",
       " 'porno.',\n",
       " 'pornographic',\n",
       " 'pornographic.',\n",
       " 'potatoes',\n",
       " 'pun',\n",
       " 'purposes',\n",
       " 'race',\n",
       " 'rather',\n",
       " 'reality',\n",
       " 'really',\n",
       " 'really,',\n",
       " 'released',\n",
       " 'rented',\n",
       " 'scenes',\n",
       " 'see',\n",
       " 'seized',\n",
       " 'sex',\n",
       " 'she',\n",
       " 'shock',\n",
       " 'shocking,',\n",
       " 'shot',\n",
       " 'shown',\n",
       " 'some',\n",
       " 'sort',\n",
       " 'staple',\n",
       " 'states.',\n",
       " 'stockholm',\n",
       " 'store',\n",
       " 'student',\n",
       " 'study',\n",
       " 'such',\n",
       " 'surrounded',\n",
       " 'swede',\n",
       " 'swedish',\n",
       " 'teacher,',\n",
       " 'than',\n",
       " 'that',\n",
       " 'the',\n",
       " 'theaters',\n",
       " 'their',\n",
       " 'then',\n",
       " 'therefore',\n",
       " 'this',\n",
       " 'thought',\n",
       " 'to',\n",
       " 'tried',\n",
       " 'u.s.',\n",
       " 'united',\n",
       " 'video',\n",
       " 'vietnam',\n",
       " 'wanting',\n",
       " 'wants',\n",
       " 'war',\n",
       " 'was',\n",
       " 'what',\n",
       " 'when',\n",
       " 'while',\n",
       " 'who',\n",
       " 'with',\n",
       " 'years',\n",
       " 'young'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w0 = set([o.lower() for o in trn[0].split(' ')]); w0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "185"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(w0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53936"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "veczr.vocabulary_['really']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_term_doc[0,53936]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_term_doc[0,5000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Theory break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This is to create a markdown style table from a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|                              |   bad |   ever |   great |   i |   is |   liked |   movie |   the |   this |   was |   worst |\n",
      "|:-----------------------------|------:|-------:|--------:|----:|-----:|--------:|--------:|------:|-------:|------:|--------:|\n",
      "| This movie was great         |     0 |      0 |       1 |   0 |    0 |       0 |       1 |     0 |      1 |     1 |       0 |\n",
      "| I liked this movie           |     0 |      0 |       0 |   1 |    0 |       1 |       1 |     0 |      1 |     0 |       0 |\n",
      "| This is the worst movie ever |     0 |      1 |       0 |   0 |    1 |       0 |       1 |     1 |      1 |     0 |       1 |\n",
      "| Bad movie                    |     1 |      0 |       0 |   0 |    0 |       0 |       1 |     0 |      0 |     0 |       0 |\n"
     ]
    }
   ],
   "source": [
    "tmp = []\n",
    "tmp.append('This movie was great')\n",
    "tmp.append('I liked this movie')\n",
    "tmp.append('This is the worst movie ever')\n",
    "tmp.append('Bad movie')\n",
    "veczr_tmp = CountVectorizer(tokenizer=tokenize)\n",
    "tmp_fit = veczr_tmp.fit_transform(tmp)\n",
    "df = pd.DataFrame(tmp_fit.toarray(),columns=veczr_tmp.get_feature_names(),index=tmp)\n",
    "\n",
    "from tabulate import tabulate\n",
    "print(tabulate(df, headers=df.columns,tablefmt=\"pipe\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In general, I want to know, given a specific document (which in our case refers to a review), whether it is a positive or negative review, class 0 or class 1, respectively.  Using Bayes I can determine the probability that a specific document will be in a certain class, e.g., $p\\left(c=1\\mid d\\right)$. \n",
    "\n",
    "$$ p\\left(c=1 \\mid d\\right) = \\dfrac{p\\left(d\\mid c=1\\right) p\\left(c=1\\right)}{p\\left(d\\right)} $$\n",
    "\n",
    "Let's take this one step further before trying to solve, as it will make the math easier.  I don't realy care about the probabilty or a review being positive or negative, I just want to know if it's more likely to be psoitive or negative.  I can extract this information by taking the ratio of the conditional probabilities.\n",
    "\n",
    "$$\\dfrac{p\\left(c=1 \\mid d\\right)}{p\\left(c=0 \\mid d\\right)} $$\n",
    "\n",
    "If the result is greater than 1, then the review is more likely to belong to class 1, i.e., positive and if the result is less than 1, the review is more likely to be negative, i.e., class 0.\n",
    "\n",
    "\\begin{align} \n",
    "\\dfrac{p\\left(c=1 \\mid d\\right)}{p\\left(c=0 \\mid d\\right)} & = \\dfrac{p\\left(d\\mid c=1\\right) p\\left(c=1\\right)}{p\\left(d\\right)} \\dfrac{p\\left(d\\right)}{p\\left(d\\mid c=0\\right) p\\left(c=0\\right)} \\\\\n",
    "& = \\dfrac{p\\left(d\\mid c=1\\right) p\\left(c=1\\right)}{p\\left(d\\mid c=0\\right) p\\left(c=0\\right)} \\\\\n",
    "\\end{align}\n",
    "\n",
    "Let's go through each of these terms in the context of the four sample reviews in the following term document matrix.\n",
    "\n",
    "\n",
    "\n",
    "|                              |   bad |   ever |   great |   i |   is |   liked |   movie |   the |   this |   was |   worst |\n",
    "|:-----------------------------|------:|-------:|--------:|----:|-----:|--------:|--------:|------:|-------:|------:|--------:|\n",
    "| This movie was great         |     0 |      0 |       1 |   0 |    0 |       0 |       1 |     0 |      1 |     1 |       0 |\n",
    "| I liked this movie           |     0 |      0 |       0 |   1 |    0 |       1 |       1 |     0 |      1 |     0 |       0 |\n",
    "| This is the worst movie ever |     0 |      1 |       0 |   0 |    1 |       0 |       1 |     1 |      1 |     0 |       1 |\n",
    "| Bad movie                    |     1 |      0 |       0 |   0 |    0 |       0 |       1 |     0 |      0 |     0 |       0 |\n",
    "\n",
    "$p\\left(c=C\\right)$ is simply the probability of a document being class 0 or 1.  This is just the number of docuemnts in each class divided by the total number of documents\n",
    "\n",
    "\\begin{align}\n",
    "p\\left(c=0\\right) &=2/4 = 0.5 \\\\\n",
    "p\\left(c=1\\right) &= 2/4 = 0.5 \\\\\n",
    "\\end{align}\n",
    "\n",
    "$p\\left(d\\mid c=C\\right)$ is the probability of seeing this document given a specific class, $C$.  Since the document is just the words (or in nlp speak, the features) which make it up, we can rewrite these terms as $p\\left(f_0,f_1,\\ldots,f_p\\mid c=0\\right)$.  For example, for the first review \n",
    "\n",
    "$$ p\\left(d_0\\mid c=0\\right) = p\\left(f_8, f_6, f_9, f_2\\mid c=0\\right)$$\n",
    "\n",
    "Here is where the Naive part of Naive Bayes comes in.  In Naive Bayes we assume that all features are conditionally independent, which means that I can rewrite the previous equation as \n",
    "\n",
    "$$ p\\left(f_8, f_6, f_9, f_2\\mid c=C\\right) = p\\left(f_8 \\mid c=C\\right) \\times p\\left(f_6 \\mid c=C\\right) \\times p\\left(f_9 \\mid c=0\\right) \\times p\\left(f_2 \\mid c=C\\right) = \\prod\\limits_{i=8,6,9,2}p\\left(f_i \\mid c=C\\right) $$\n",
    "\n",
    "Going back to our problem, we can now calculate $p\\left(d_0 \\mid c=0\\right)$ for each feature as the number of times that feature appears in the document divided by the number of times that feature appears in all documents of that class.\n",
    "\n",
    "$$p\\left(f_{this}\\mid c=0\\right) = 2/2 = 1$$\n",
    "\n",
    "$$p\\left(f_{movie}\\mid c=0\\right) = 2/2 = 1$$\n",
    "\n",
    "$$p\\left(f_{was}\\mid c=0\\right) = 1/2 = 0.5$$\n",
    "\n",
    "$$p\\left(f_{great}\\mid c=0\\right) = 1/2 = 0.5$$\n",
    "\n",
    "$$p\\left(d_0 \\mid c=0\\right) = 1\\times 1 \\times 0.5 \\times 0.5 = 0.25 $$\n",
    "\n",
    "If we try and repeat the same procedure for the other class, we end up with a problem.  What happens if that feature never appears in that class.\n",
    "\n",
    "$$p\\left(f_{great}\\mid c=1\\right) = 1/0 = ???$$\n",
    "\n",
    "To get around this problem we add an additional row to our term document matrix which contains a 1 in every entry.  Intuitively this row represents the idea that there is never a zero percent chance of some word appearing.  It might be infinitesimal, but it is greater than zero. This row of ones is used just for calculating $p\\left(d \\mid c=C\\right)$\n",
    "\n",
    "|                              |   bad |   ever |   great |   i |   is |   liked |   movie |   the |   this |   was |   worst |\n",
    "|:-----------------------------|------:|-------:|--------:|----:|-----:|--------:|--------:|------:|-------:|------:|--------:|\n",
    "| This movie was great         |     0 |      0 |       1 |   0 |    0 |       0 |       1 |     0 |      1 |     1 |       0 |\n",
    "| I liked this movie           |     0 |      0 |       0 |   1 |    0 |       1 |       1 |     0 |      1 |     0 |       0 |\n",
    "| This is the worst movie ever |     0 |      1 |       0 |   0 |    1 |       0 |       1 |     1 |      1 |     0 |       1 |\n",
    "| Bad movie                    |     1 |      0 |       0 |   0 |    0 |       0 |       1 |     0 |      0 |     0 |       0 |\n",
    "| **ones**                     |     1 |      1 |       1 |   1 |    1 |       1 |       1 |     1 |      1 |     1 |       1 |\n",
    "\n",
    "We can now recalculate the probabilities:\n",
    "\n",
    "$$p\\left(f_{this}\\mid c=0\\right) = (2+1)/3 = 1$$\n",
    "\n",
    "$$p\\left(f_{movie}\\mid c=0\\right) = (2+1)/3 = 1$$\n",
    "\n",
    "$$p\\left(f_{was}\\mid c=0\\right) = (1+1)/3 = 0.667$$\n",
    "\n",
    "$$p\\left(f_{great}\\mid c=0\\right) = (1+1)/3 = 0.667$$\n",
    "\n",
    "$$p\\left(d_0 \\mid c=0\\right) = 1\\times 1 \\times 0.667 \\times 0.667 = 0.444 $$\n",
    "\n",
    "Repeat for the other class\n",
    "\n",
    "$$p\\left(f_{this}\\mid c=1\\right) = (1+1)/3 = 0.667$$\n",
    "\n",
    "$$p\\left(f_{movie}\\mid c=1\\right) = (2+1)/3 = 1$$\n",
    "\n",
    "$$p\\left(f_{was}\\mid c=1\\right) = (0+1)/3 = 0.333$$\n",
    "\n",
    "$$p\\left(f_{great}\\mid c=1\\right) = (0+1)/3 = 0.333$$\n",
    "\n",
    "$$p\\left(d_0 \\mid c=1\\right) = 0.667 \\times 1 \\times 0.333 \\times 0.333 = 0.074 $$\n",
    "\n",
    "Now to answer our original question we just need to take the ratios of these two probabilities\n",
    "\n",
    "$$ \\dfrac{p\\left(d_0 \\mid c=0\\right)}{p\\left(d_0 \\mid c=1\\right)} = \\dfrac{0.444}{0.074} = 6$$\n",
    "\n",
    "This tells us that it is 6 times more likely that review 1 belongs to class 0, than class 1.\n",
    "\n",
    "As an aside: If we didn't use Naive Bayes the equation would be much harder to solve as the $p\\left(f_8, f_6, f_9, f_2\\mid c=0\\right)$ would expand into a much harder term to solve:\n",
    "\n",
    "$$ p\\left(f_8, f_6, f_9, f_2\\mid c=0\\right) = p\\left(f_8 \\mid c=0\\right) \\times p\\left(f_6\\mid f_8, c=0\\right) \\times p\\left(f_9, f_2\\mid f_8, f_6, c=0\\right) \\times p\\left(f_2\\mid f_8, f_6, f_9, c=0\\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theory break over"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As stated above, the probability of each feature appearing in a document of a specific class is just the ratio of the number of times that feature appears in that class (plus 1) to the number of documents in that class (plus 1).  The ratio of ratios is then half of the equation we need to solve the problem.  To make the multiplication and division less likely to go to zero or infinity we can also convert everything to logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pr(y_i):\n",
    "    p = x[y==y_i].sum(0)\n",
    "    return (p+1) / ((y==y_i).sum()+1)\n",
    "\n",
    "x=trn_term_doc\n",
    "y=trn_y\n",
    "\n",
    "r = np.log(pr(1)/pr(0))\n",
    "b = np.log((y==1).mean() / (y==0).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the formula for Naive Bayes.\n",
    "\n",
    "Instead of calculating the probabilities for each document individiually we can just use matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81655999999999995"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_preds = val_term_doc @ r.T + b\n",
    "preds = pre_preds.T>0\n",
    "(preds==val_y).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and binarized Naive Bayes (where I don't care how often I've seen it, just if I have seen it or not)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83016000000000001"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=trn_term_doc.sign()\n",
    "r = np.log(pr(1)/pr(0))\n",
    "\n",
    "pre_preds = val_term_doc.sign() @ r.T + b\n",
    "preds = pre_preds.T>0\n",
    "(preds==val_y).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how we can fit logistic regression where the features are the unigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83328000000000002"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = LogisticRegression(C=1e8, dual=True)\n",
    "m.fit(x, y)\n",
    "preds = m.predict(val_term_doc)\n",
    "(preds==val_y).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and the binarized version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85519999999999996"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = LogisticRegression(C=1e8, dual=True)\n",
    "m.fit(trn_term_doc.sign(), y)\n",
    "preds = m.predict(val_term_doc.sign())\n",
    "(preds==val_y).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and the regularized version (The C paramater, the closer to 1, the greater the regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84872000000000003"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = LogisticRegression(C=0.1, dual=True)\n",
    "m.fit(x, y)\n",
    "preds = m.predict(val_term_doc)\n",
    "(preds==val_y).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and the regularized binarized version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88404000000000005"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = LogisticRegression(C=0.1, dual=True)\n",
    "m.fit(trn_term_doc.sign(), y)\n",
    "preds = m.predict(val_term_doc.sign())\n",
    "(preds==val_y).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigram with NB features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next model is a version of logistic regression with Naive Bayes features described [here](https://www.aclweb.org/anthology/P12-2018). For every document we compute binarized features as described above, but this time we use bigrams and trigrams too. Each feature is a log-count ratio. A logistic regression model is then trained to predict sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "veczr =  CountVectorizer(ngram_range=(1,3), tokenizer=tokenize, max_features=800000)\n",
    "trn_term_doc = veczr.fit_transform(trn)\n",
    "val_term_doc = veczr.transform(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 800000)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_term_doc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = veczr.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['by vast', 'by vengeance', 'by vengeance .', 'by vera', 'by vera miles']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[200000:200005]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=trn_y\n",
    "x=trn_term_doc.sign()\n",
    "val_x = val_term_doc.sign()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.log(pr(1) / pr(0))\n",
    "b = np.log((y==1).mean() / (y==0).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we fit regularized logistic regression where the features are the trigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90500000000000003"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = LogisticRegression(C=0.1, dual=True)\n",
    "m.fit(x, y);\n",
    "\n",
    "preds = m.predict(val_x)\n",
    "(preds.T==val_y).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the $\\text{log-count ratio}$ `r`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 800000), matrix([[-0.05468386, -0.16100472, -0.24783616, ...,  1.09861229,\n",
       "          -0.69314718, -0.69314718]]))"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.shape, r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 0.94678442,  0.85128806,  0.7804878 , ...,  3.        ,\n",
       "          0.5       ,  0.5       ]])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we fit regularized logistic regression where the features are the trigrams' log-count ratios.\n",
    "\n",
    "This is not equivalent to just multiplying the weight by the ratios as the weights get regularized while the input values do not.  Thus when you multiply the input values by the naive Bayes ratios you are essentially saying that you beleive the ratios and that the model should not alter them unless it has a good reason to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.91768000000000005"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_nb = x.multiply(r)\n",
    "m = LogisticRegression(dual=True, C=0.1)\n",
    "m.fit(x_nb, y);\n",
    "\n",
    "val_x_nb = val_x.multiply(r)\n",
    "preds = m.predict(val_x_nb)\n",
    "(preds.T==val_y).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fastai NBSVM++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "sl=2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is how we get a model from a bag of words\n",
    "md = TextClassifierData.from_bow(trn_term_doc, trn_y, val_term_doc, val_y, sl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "075afdb7ebee432bb4917146014ebfcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.       0.0251   0.12003  0.91552]                          \n",
      "\n"
     ]
    }
   ],
   "source": [
    "learner = md.dotprod_nb_learner()\n",
    "learner.fit(0.02, 1, wds=1e-6, cycle_len=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbcc4a674a5a46fca621e6d2244d9511",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.       0.02014  0.11387  0.92012]                         \n",
      "[ 1.       0.01275  0.11149  0.92124]                         \n",
      "\n"
     ]
    }
   ],
   "source": [
    "learner.fit(0.02, 2, wds=1e-6, cycle_len=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6010f5e493984a6991da1c3e7079e72a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.       0.01681  0.11089  0.92129]                           \n",
      "[ 1.       0.00949  0.10951  0.92223]                          \n",
      "\n"
     ]
    }
   ],
   "source": [
    "learner.fit(0.02, 2, wds=1e-6, cycle_len=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* Baselines and Bigrams: Simple, Good Sentiment and Topic Classification. Sida Wang and Christopher D. Manning [pdf](https://www.aclweb.org/anthology/P12-2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "104px",
    "width": "251px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
