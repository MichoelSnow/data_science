{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Bag of Words (CBOW)\n",
    "Or, how to predict a word given its context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notation\n",
    "\n",
    "- $w$ - word\n",
    "- $x$ - input vector\n",
    "- $x_k$ - non-zero element of input vector\n",
    "- $V$ - numbers of words in vocabulary\n",
    "- $W$ - input weight matrix\n",
    "- $W^{\\prime}$ - output weight matrix\n",
    "- $h$ - hidden layer vector\n",
    "- $N$ - number of neurons in the hidden layer\n",
    "- $y$ - output vector\n",
    "- $C$ - number of context words\n",
    "\n",
    "\n",
    "### Layers\n",
    "\n",
    "- Input layer ($x$) - vector  - $V\\times1$ one-hot input vector of context word\n",
    "- Hidden Layer ($h$) - vector - $N \\times 1$ \n",
    "- Output Layer ($y$) - vector - $V\\times1$ output vector of predicted word whose values sum to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single word context\n",
    "\n",
    "i.e., $C=1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/CBOW_viz-01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/CBOW_viz-02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encoding of $x$ results in only a single column of the matrix multiplication ($W^T \\times x$) output having non-zero values.  This column of $W^T$ corresponds to the $k^{th}$ row of $W$.  Since the hidden layer is the result of this multiplication:\n",
    "\n",
    "$$h = {W_{k,:}}^T := v_c$$ \n",
    "\n",
    "$v_c$ is just the variable name which corresponds to the context vector ${W_{k,:}}^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/CBOW_viz-03.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hidden layer $h$ is then multiplied by the output weight matrix $(W^{\\prime T}\\times h)$ to get the linear form of the output vector $u$.  Each row of that column vector corresponds to a single word in the vocabulary. The corresponding column in the output weight matrix, $W^{\\prime}_{:,i}$, is referred to as the word vector $v_{w_i}$. Note the difference between $w$, which refers to a word in vocabulary, and $W$ as well as $W^{\\prime}$, which refer to the weight matrices.\n",
    "\n",
    "$u$ is the linear component of the output vector, formed by the product of the word and context vectors, ${v_{w_i}}^T v_c$.  In order to convert the linear output into a probability, for which the sum of the vector is $1$, A non-linear transformation is applied (softmax).  Each value in the output vector $y$, corresponds to the probability of the that word being the predicted word given the context. \n",
    "\n",
    "$$p(w_i\\mid c) = \\dfrac{\\exp{u_i}}{\\sum\\limits_{j=1}^V\\exp{u_j}} = \\dfrac{\\exp {v_{w_i}}^Tv_c}{\\sum\\limits_{j=1}^V\\exp {v_{w_j}}^Tv_c}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple word context\n",
    "\n",
    "i.e., $C>1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/CBOW_viz-04.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/CBOW_viz-05.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In multiple word context CBOW there are multiple one-hot input vectors (one for each context word).  There is still only a single input weight matrix, $W$, which is matrix mutliplied by the average input vector, i.e., $W^T \\times \\frac{1}{C}\\sum\\limits_{i=1}^Cx^i$.  Since this is the summation of multiple one hot vector, instead of there being only a single value from each column of the transposed weight matrix, there is one value for each input vector.  Put another way, for a context of $C$ words, each context vector has only a single value of 1 and the rest of their values are zero.  I can represent the index of that singular 1 value as $x^1_k$ for the first word, $x^2_k$ for the second word, and so on, through $x^C_k$.  Since their values are 1, I only care about the list of indices.  I then find the corresponding column indices in the input weight matrix, and take the average, which gives me the value for the hidden layer of that neuron. \n",
    "\n",
    "The rest of the CBOW model continues the same as it did for the single word context CBOW explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to learn the correct values for the weight matrices you need to use backpropagation, which requires a loss function to guide it. There are multiple ways of calculating loss functions for this formulation, presented below are the most commonly used ones. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross entropy comes from information theory and describes the average number of bits required to identify an event (or in our case a word), if we use a coding scheme optimized for an \"unnatural\" distribution, rather than the true distribution.  The more different the two distrubtions are, the more bits required to describe the event.  In terms of a loss function, the cross entropy is a measure of how different the model distribution is from the true distribution, based on the average number of bits the model needs to describe an event drawn from the true distribution.  When the cross entropy is zero, then the model distribution perfectly models the true distribution.  This is why we can use cross entropy as a loss function and why we want to minimize the cross entropy.  \n",
    "\n",
    "For discrete distributions the formulation for cross entropy is:\n",
    "\n",
    "$$ H(p,q) = -\\sum\\limits_{x}p(x)\\log q(x)$$\n",
    "\n",
    "Where $p$ is the true distribution, $y$, and $q$ is the model distribution, $p(w_i \\mid c)$.  \n",
    "\n",
    "$$\\mathcal{L}_{\\theta} = -\\sum\\limits_{i=1}^Vy_i\\log p(w_i\\mid c)$$\n",
    "\n",
    "Since in the true distribution there is only one correct word for the context, $y$ is one hot encoded and $y_i=0$ for all values of $i$ **except** when $w_i$ is the output word, denoted by $w_o$. \n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal{L}_{\\theta}  &= -\\sum\\limits_{i=1}^Vy_i\\log p(w_i\\mid c) \\\\\n",
    "&=-\\log p(w_o\\mid c) \\\\\n",
    "&= -log\\dfrac{\\exp{u_o}}{\\sum\\limits_{j=1}^V\\exp{u_j}} \\\\\n",
    "&= -u_o + \\log \\sum\\limits_{j=1}^V\\exp\\left(u_j\\right) \\\\\n",
    "&= -{v_{w_o}}^Tv_c + \\log \\sum\\limits_{j=1}^V\\exp\\left( {v_{w_j}}^Tv_c\\right) := E\n",
    "\\end{align}\n",
    "\n",
    "Given this likelihood function, in order to get the correct weight matrices (or, as done in this case, their corresponding vectors) starting from random values we need to use gradient descent, which requires solving for the gradient of the loss function with respect to the two weight matrices.  Starting with the output weight matrix.\n",
    "\n",
    "\\begin{align}\n",
    "\\dfrac{\\partial E}{\\partial v_{w_i}} &= \\dfrac{\\partial E}{\\partial u_i}\\dfrac{\\partial u_i}{\\partial v_{w_i}}\\\\\n",
    "&=\\left(-\\dfrac{\\partial u_o}{\\partial u_i} +\\dfrac{\\dfrac{\\partial \\sum\\limits_{j=1}^V\\exp\\left(u_j\\right)}{\\partial u_i}}{\\sum\\limits_{j=1}^V\\exp\\left(u_j\\right)}\\right)\\left(\\dfrac{\\partial{v_{w_i}}^Tv_c }{\\partial v_{w_i}}\\right) \\\\\n",
    "&=\\left(-\\delta_i +\\dfrac{\\frac{\\partial \\exp(u_1)}{\\partial u_i} + \\frac{\\partial \\exp(u_1)}{\\partial u_i} + \\ldots + \\frac{\\partial \\exp(u_i)}{\\partial u_i} + \\ldots + \\frac{\\partial \\exp(u_V)}{\\partial u_i}}{{\\sum\\limits_{j=1}^V\\exp\\left(u_j\\right)}}\\right)v_c \\\\\n",
    "&= \\left(-\\delta_i + \\dfrac{\\exp(u_i)\\dfrac{\\partial}{\\partial u_i}u_i}{{\\sum\\limits_{j=1}^V\\exp\\left(u_j\\right)}}\\right)h_i \\\\\n",
    "&= \\left(-\\delta_i + p(w_i\\mid c) \\right)h_i \\\\\n",
    "&= \\left(y_i - \\delta_i\\right) h_i\n",
    "\\end{align}\n",
    "\n",
    "Where $\\delta_i$ is the delta function, i.e., it has a value of $1$ when $i^{th}$ is the acutal output word, $w_o$, otherwise its value is $0$.  Plugging this into the gradient descent equation:\n",
    "\n",
    "$$ {v_{w_i}}^{new} = {v_{w_i}}^{old} - \\eta \\left(y_i - \\delta_i\\right) h_i $$\n",
    "\n",
    "or, alternatively:\n",
    "\n",
    "$$ {W^{\\prime}_{:,i}}^{new} = {W^{\\prime}_{:,i}}^{old} - \\eta \\left(y_i - \\delta_i\\right) h_i $$\n",
    "\n",
    "Now to derive the update equation for the input weight matrix.\n",
    "\n",
    "\\begin{align}\n",
    "\\dfrac{\\partial E}{\\partial W^i_{k,n}} &= \\sum\\limits_{j=1}^V\\dfrac{\\partial E}{\\partial u_j}\\dfrac{\\partial u_j}{\\partial h_n}\\dfrac{\\partial h_n}{\\partial W^i_{k,n}}\\\\\n",
    "&= \\sum\\limits_{j=1}^V \\left(y_j - \\delta_j\\right)\\left({v_{w_j}}^T\\right)\\left( \\dfrac{\\partial}{\\partial W^i_{k,n}} \\dfrac{1}{C}\\sum\\limits_{i=1}^C{W^i_{k,n}}\\right)\\\\\n",
    "&= \\sum\\limits_{j=1}^V \\left(y_j - \\delta_j\\right)\\left({v_{w_j}}^T\\right)\\left(1\\right)\\\\\n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of single context CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding\n",
    "def onehot(sentence):\n",
    "    sent_clean = re.sub(\"[^a-zA-Z\\s\\']+\", \"\", sentence)\n",
    "    words = sent_clean.lower().split(\" \")\n",
    "    words_unq = list(set(words))\n",
    "    words_df = pd.DataFrame(np.identity(len(words_unq)),columns = words_unq)\n",
    "    return words_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way\"\n",
    "words_df = onehot(sent)\n",
    "words_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's choose a hidden layer of 3 neurons (although you can choose one of any size)<br>\n",
    "\n",
    "For the weight matrices you want to initiliaze them to some small random values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = len(words_df)\n",
    "N = 3\n",
    "Wi = np.random.rand(V,N) - 0.5\n",
    "Wo = np.random.rand(N,V) - 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's predict the next word given the previous word \"worst\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_one_context(df,word,Wi,Wo):\n",
    "    h = np.dot(np.transpose(Wi),words_df[word].as_matrix())\n",
    "    y_act = np.dot(np.transpose(Wo),h)\n",
    "    y_act_full = np.dot(np.transpose(Wo),np.transpose(Wi))\n",
    "    y_num = np.exp(y_act)\n",
    "    y_denom = np.sum(np.exp(y_act_full),1)\n",
    "    return y_num/y_denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"worst\"\n",
    "word_pred = \"of\"\n",
    "y_i = predict_one_context(words_df,word,Wi,Wo)\n",
    "y_i[words_df.columns.get_loc(word_pred)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial weight matrices are generated at random.  To learn the proper weight, you need to to maximize the probability likelihood, using the word and context vectors as the paramters, i.e. $\\theta = \\{{v_{w_i}},v_c\\}$.  However, since we are working with a loss function we can minimize the negative likelihood and use the log likelihood to make things easier.\n",
    "\n",
    "\\begin{align}\n",
    "\\ell\\{\\theta\\} &= -\\log p(w_i\\mid c;\\theta) \\\\\n",
    "&= -\\log \\prod\\limits\\\\\n",
    "\\end{align}\n",
    "\n",
    "$$L(\\theta) = \\prod\\limits_{i=1}^Vp(w_i\\mid c;\\theta) = \\prod\\limits_{i=1}^V \\dfrac{\\exp {v_{w_i}}^Tv_c}{\\sum\\limits_{j=1}^V\\exp {v_{w_j}}^Tv_c}$$\n",
    "\n",
    "taking the log\n",
    "\n",
    "$$\\ell(\\theta)  = \\sum\\limits_{i=1}^V {v_{w_i}}^Tv_c - \\sum\\limits_{i=1}^V \\sum\\limits_{j=1}^V\\exp {v_{w_j}}^Tv_c$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
