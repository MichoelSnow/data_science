{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is based off of the [SGD mnist](https://github.com/fastai/fastai/blob/master/courses/ml1/lesson4-mnist_sgd.ipynb) lesson from fastai \n",
    "\n",
    "In this notebook we will start with a pytorch neural network implementation of logitistic regression and then program it ourselves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_sci.imports import *\n",
    "from data_sci.utilities import *\n",
    "from data_sci.fastai import *\n",
    "from data_sci.fastai.dataset import *\n",
    "from data_sci.fastai.metrics import *\n",
    "from data_sci.fastai.torch_imports import *\n",
    "from data_sci.fastai.model import *\n",
    "\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Path to download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '/data/msnow/data_science/mnist/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# MNIST Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's download, unzip, and format the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "URL='http://deeplearning.net/data/mnist/'\n",
    "FILENAME='mnist.pkl.gz'\n",
    "\n",
    "def load_mnist(filename):\n",
    "    return pickle.load(gzip.open(filename, 'rb'), encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "get_data(os.path.join(URL,FILENAME), os.path.join(PATH,FILENAME))\n",
    "((x, y), (x_valid, y_valid), _) = load_mnist(os.path.join(PATH,FILENAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, (50000, 784), numpy.ndarray, (50000,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x), x.shape, type(y), y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Many machine learning algorithms behave better when the data is *normalized*, that is when the mean is 0 and the standard deviation is 1. We will subtract off the mean and standard deviation from our training set in order to normalize the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.13044983, 0.30728981, -3.1638146e-07, 0.99999934)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = x.mean()\n",
    "std = x.std()\n",
    "\n",
    "x=(x-mean)/std\n",
    "mean, std, x.mean(), x.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Note that for consistency (with the parameters we learn when training), we subtract the mean and standard deviation of our training set from our validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.0058509219, 0.99243325)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_valid = (x_valid-mean)/std\n",
    "x_valid.mean(), x_valid.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def plots(ims, figsize=(12,6), rows=2, titles=None):\n",
    "    f = plt.figure(figsize=figsize)\n",
    "    cols = len(ims)//rows\n",
    "    for i in range(len(ims)):\n",
    "        sp = f.add_subplot(rows, cols, i+1)\n",
    "        sp.axis('Off')\n",
    "        if titles is not None: sp.set_title(titles[i], fontsize=16)\n",
    "        plt.imshow(ims[i], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The data has been reduced from a rank 3 tensor of image number by image width by image height to a rank 2 tensor of number of image number by pixel. Before visualizing the image we need to convert it back into a rank 3 tensor using numpy's `reshape`.  \n",
    "\n",
    "`y_valid` is the list of integers which correspond to the images in `x_valid`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_imgs = np.reshape(x_valid, (-1,28,28)); x_imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_imgs[0],cmap='gray')\n",
    "plt.title(y_valid[0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plots(x_imgs[:8], titles=y_valid[:8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "A **function** takes inputs and returns outputs.  The classic example is the equation for a line \n",
    "\n",
    "$$ f(x) = ax + b $$\n",
    "\n",
    "where $x$ is the input, $a$ and $b$ are the **parameters** and $f(x)$ is the output.  A *neural network* is just a specific type of function (an *infinitely flexible function* to be exact), consisting of *layers*, which are made up of parameters.  A *layer* is a linear function such as matrix multiplication followed by a non-linear function (the *activation*).  For example, an equation for a simple neural network can be represented with the equation:\n",
    "\n",
    "$$ f(\\mathbf{x}) = a_{nl}\\left(a_l \\mathbf{x}\\right) $$\n",
    "\n",
    "where $\\mathbf{x}$ is the input (the bold font just means that it is not just a scalar, i.e., it is a tensor of rank greater than 0), $a_l$ is the linear layer parameter, and $a_{nl}$ is the non linear layer parameter.\n",
    "\n",
    "However, most neural networks are not nearly this simple.  They often have thousands, or even hundreds of thousands of parameters.  However the core idea is the same.  The neural network is a function, and we will learn the best parameters for modeling our data.\n",
    "\n",
    "As an aside there always needs to be a non-linear function after each linear function, as multiple linear functions in a row can always be represented by a single linear function.  This is part of the definition of linear transformations.  For example, multiplying anything by 5 and then dividing the result by 2 is equivalent to multiplying the input by 5/2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "PyTorch has two overlapping, yet distinct, purposes.  As described in the [PyTorch documentation](http://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html):\n",
    "\n",
    "<img src=\"images/what_is_pytorch.png\" alt=\"pytorch\" style=\"width: 80%\"/>\n",
    "\n",
    "The neural network functionality of PyTorch is built on top of the Numpy-like functionality for fast matrix computations on a GPU. Although the neural network purpose receives way more attention, both are very useful.  We'll implement a neural net from scratch today using PyTorch.\n",
    "\n",
    "**Further learning**: If you are curious to learn what *dynamic* neural networks are, you may want to watch [this talk](https://www.youtube.com/watch?v=Z15cBAuY7Sc) by Soumith Chintala, Facebook AI researcher and core PyTorch contributor.\n",
    "\n",
    "If you want to learn more PyTorch, you can try this [introductory tutorial](http://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html) or this [tutorial to learn by examples](http://pytorch.org/tutorials/beginner/pytorch_with_examples.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use the built in Neural Net for Logistic Regression in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will begin with the highest level abstraction: using a neural net defined by PyTorch's Sequential class.  In PyTorch to switch between cpu and CUDA enabled gpu you just need to add `.cuda()` to the end of any nn architecture:\n",
    "\n",
    "- GPU enabled\n",
    "```python\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(28*28, 10),\n",
    "    nn.LogSoftmax()\n",
    ").cuda()\n",
    "```\n",
    "\n",
    "- GPU disabled (using cpu)\n",
    "```python\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(28*28, 10),\n",
    "    nn.LogSoftmax()\n",
    ")\n",
    "```\n",
    "\n",
    "As we said above, each layer of the neural network is just the data modified first by a linear function and then by a non-linear function.  So if we wanted to build a neural network version of multinomial logistic regression, termed softmax in neural networks. Just as a reminder, here is the equation for softmax to predict the $j$-th class given a sample vector $\\mathbf{x}$ and a weighting vector $\\mathbf{w}$:\n",
    "\n",
    "$$ Pr(y=j \\mid  \\mathbf{x}) = \\dfrac{\\exp\\left(\\mathbf{x}^T \\mathbf{w}_j\\right)}{\\sum\\limits_{k=1}^K\\exp\\left(\\mathbf{x}^T\\mathbf{w}_k\\right)}$$\n",
    "\n",
    "Each input is a vector of size `28*28` pixels and our output is of size `10` (since there are 10 digits: 0, 1, ..., 9). \n",
    "\n",
    "We use the output of the final layer to generate our predictions.  Often for classification problems (like MNIST digit classification), the final layer has the same number of outputs as there are classes.  In that case, this is 10: one for each digit from 0 to 9.  These can be converted to comparative probabilities.  For instance, it may be determined that a particular hand-written image is 80% likely to be a 4, 18% likely to be a 9, and 2% likely to be a 3.\n",
    "\n",
    "So putting everything together, we want a layer composed of a linear component which converts our `28*28` rank 1 tensor (aka vector) inputs into an output rank 1 tensor of size `10` and then we want to apply a softmax classification function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(\n",
    "    nn.Linear(28*28, 10),\n",
    "    nn.LogSoftmax()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = ImageClassifierData.from_arrays(PATH, (x,y), (x_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ImageClassifierData` is from the [Fast AI library](https://github.com/fastai/fastai/blob/master/fastai/dataset.py) and calls a PyTorch data loader, which grabs a few images, sticks them into a mini-batch and makes them available, this is equivalent to a python generator.  You can call each of the different data loaders\n",
    "\n",
    "- training data loader = md.trn_dl\n",
    "- validation data loader = md.val_dl\n",
    "- test data loader = md.test_dl\n",
    "- augmented data loader = md.aug_dl\n",
    "- test augemented data loader = md.test_aug_dl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Aside about loss functions and metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In machine learning the **loss** function or cost function is representing the price paid for inaccuracy of predictions.  To understand where this loss function comes from let's take a little detour into probability theory. \n",
    "\n",
    "Given a fair coin what is the probability of the following outcomes:\n",
    " - Tails, Heads\n",
    " - Heads, Heads\n",
    " - Tails, Tails\n",
    "You don't need any equations to know that the probability of each of those outcomes is 0.25, as there are four possible outcomes when you flip a coin twice, and if it is a fair coin the probability of each outcome is 0.25.  We can formalize this through the probability mass function (pmf) of the Bernoulli distribution:\n",
    "\n",
    "$$\n",
    "f(y;p) = p^y\\left(1-p\\right)^{1-y} \\texttt{ for }k \\in \\{0,1\\}\n",
    "$$\n",
    "\n",
    "where $y$ is the number of events occuring, $p$ is the probability of that event occuring and $f(y;p)$ is the probability of $y$ events occuring given $p$.  One way to determine the loss of this function is to take the negative log-likelihood (or cross-entropy) of our function.\n",
    "\n",
    "\\begin{align}\n",
    "\\ell\\left(f\\right) & = \\log \\left(p^y\\left(1-p\\right)^{1-y}\\right)\\\\\n",
    "&= \\log \\left(p^y\\right) + \\log\\left(\\left(1-p\\right)^{1-y}\\right)\\\\\n",
    "&= y\\log p + (1-y) \\log (1-p)\n",
    "\\end{align}\n",
    "\n",
    "In machine learning terms the equations are the same, but the terms means slightly different things.  For a given observation, $y$ is the true label and $p$ is the probability given by the model's prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def binary_loss(y, p):\n",
    "    return np.mean(-(y * np.log(p) + (1-y)*np.log(1-p)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.164252033486018"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acts = np.array([1, 0, 0, 1])\n",
    "preds = np.array([0.9, 0.1, 0.2, 0.8])\n",
    "binary_loss(acts, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Note that in our toy example above our accuracy is 100% and our loss is 0.16. \n",
    "\n",
    "Why not just maximize accuracy? The binary classification loss is an easier function to optimize.\n",
    "\n",
    "For multi-class classification, we use *negative log liklihood* (also known as *categorical cross entropy*) which is exactly the same thing, but summed up over all classes.  For example, let's say we were trying to train a model to recognize pictures of animals, and the options were horse, dog, cat, lion, tiger, and bear.  We can represent each of the different options as a one hot encoded vectors:\n",
    "\n",
    "|    |   horse |   dog |   cat |   lion |   tiger |   bear |\n",
    "|---:|--------:|------:|------:|-------:|--------:|-------:|\n",
    "|  0 |       1 |     0 |     0 |      0 |       0 |      0 |\n",
    "|  1 |       0 |     1 |     0 |      0 |       0 |      0 |\n",
    "|  2 |       0 |     0 |     1 |      0 |       0 |      0 |\n",
    "|  3 |       0 |     0 |     0 |      1 |       0 |      0 |\n",
    "|  4 |       0 |     0 |     0 |      0 |       1 |      0 |\n",
    "|  5 |       0 |     0 |     0 |      0 |       0 |      1 |\n",
    "\n",
    "let's say the model predicted the following output for a bear\n",
    "\n",
    "|    |   bear |   predicted_bear |\n",
    "|---:|-------:|-----------------:|\n",
    "|  0 |      0 |             0.05 |\n",
    "|  1 |      0 |             0.05 |\n",
    "|  2 |      0 |             0.05 |\n",
    "|  3 |      0 |             0.05 |\n",
    "|  4 |      0 |             0.05 |\n",
    "|  5 |      1 |             0.75 |\n",
    "\n",
    "to calculate the loss for the prediction we look at the loss for each observation in the vector.\n",
    "\n",
    "|    |   bear |   predicted_bear | loss_equation                     |   observation_loss |\n",
    "|---:|-------:|-----------------:|:----------------------------------|-------------------:|\n",
    "|  0 |      0 |             0.05 | 0\\*log(0.05) + (1-0)\\*log(1-0.05) |          0.0512933 |\n",
    "|  1 |      0 |             0.05 | 0\\*log(0.05) + (1-0)\\*log(1-0.05) |          0.0512933 |\n",
    "|  2 |      0 |             0.05 | 0\\*log(0.05) + (1-0)\\*log(1-0.05) |          0.0512933 |\n",
    "|  3 |      0 |             0.05 | 0\\*log(0.05) + (1-0)\\*log(1-0.05) |          0.0512933 |\n",
    "|  4 |      0 |             0.05 | 0\\*log(0.05) + (1-0)\\*log(1-0.05) |          0.0512933 |\n",
    "|  5 |      1 |             0.75 | 1\\*log(0.75) + (1-1)\\*log(1-0.75) |          0.287682  |\n",
    "\n",
    "The total loss for that prediction is the sum of the individual observation losses which comes out to 0.544\n",
    "\n",
    "To understand this a little more, let's calculate the observation losses for a few more predictions\n",
    "\n",
    "\n",
    "|    |   bear |   predict_1 |    loss_1 |   predict_2 |    loss_2 |   predict_3 |   loss_3 |\n",
    "|---:|-------:|------------:|----------:|------------:|----------:|------------:|---------:|\n",
    "|  0 |      0 |        0.05 | 0.0512933 |        0.01 | 0.0100503 |         0.1 | 0.105361 |\n",
    "|  1 |      0 |        0.05 | 0.0512933 |        0.01 | 0.0100503 |         0.1 | 0.105361 |\n",
    "|  2 |      0 |        0.05 | 0.0512933 |        0.01 | 0.0100503 |         0.1 | 0.105361 |\n",
    "|  3 |      0 |        0.05 | 0.0512933 |        0.01 | 0.0100503 |         0.1 | 0.105361 |\n",
    "|  4 |      0 |        0.05 | 0.0512933 |        0.01 | 0.0100503 |         0.1 | 0.105361 |\n",
    "|  5 |      1 |        0.75 | 0.287682  |        0.95 | 0.0512933 |         0.5 | 0.693147 |\n",
    "\n",
    "You can see that as the predictions get more accurate the individual observation losses decrease and vice versa as the predictions get less accurate.\n",
    "\n",
    "This one-hot encoding for multiclass classification is why when  creating our nerual network above we built the layer to be (28,28,10).  The last value of the tuple is 10 to represent the 1 by 10 one-hot encoded vectors of the digits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now back to the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As explained in the above aside, we want to use a negative log likelihood loss function for our model, `nn.NLLLoss()` in pytorch.  When evaluating our model we can use the accuracy.  For the optimization of our model we are going to use stochastic gradient descent, telling it to use the layers in `net` as the parameters, and setting other parameters which will be explained in other notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss=nn.NLLLoss()\n",
    "metrics=[accuracy]\n",
    "opt=optim.SGD(net.parameters(), 1e-1, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Fitting* is the process by which the neural net learns the best parameters for the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "326d1d54530d4e98a3d3b8b725eb1550",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy   \n",
      "    0      1.301665   1.140233   0.8873    \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.140232567659393, 0.88729999999999998]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit(net, md, n_epochs=1, crit=loss, opt=opt, metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPUs are great at handling lots of data at once.  We break the data up into **batches**, and that specifies how many samples from our dataset we want to send to the processor (GPU or CPU) at a time.  The fastai library defaults to a batch size of 64.  On each iteration of the training loop, the error on 1 batch of data will be calculated, and the optimizer will update the parameters based on that.\n",
    "\n",
    "An **epoch** is completed once each data sample has been used once in the training loop.\n",
    "\n",
    "Now that we have the parameters for our model, we can make predictions on our validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = predict(net, md.val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our output is 10,000 by 10 because we have 10,000 observations in our validation set and 10 predictions per observation, i.e., prediction that it's a zero, prediction that it's a one, ...\n",
    "\n",
    "To see how accurate our predictions are, we want to know how often the digit highest predicted matched the actual digit.  For now, we don't care about the actual value of the prediction, just the location of the highest prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-7.747974</td>\n",
       "      <td>-3.077208</td>\n",
       "      <td>-2.283786</td>\n",
       "      <td>-0.378413</td>\n",
       "      <td>-9.398509</td>\n",
       "      <td>-3.487280</td>\n",
       "      <td>-6.933934</td>\n",
       "      <td>-12.972561</td>\n",
       "      <td>-2.002663</td>\n",
       "      <td>-10.997322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-11.389916</td>\n",
       "      <td>-7.895541</td>\n",
       "      <td>-5.798741</td>\n",
       "      <td>-4.611126</td>\n",
       "      <td>-4.631413</td>\n",
       "      <td>-3.517211</td>\n",
       "      <td>-11.018111</td>\n",
       "      <td>-9.716209</td>\n",
       "      <td>-0.054559</td>\n",
       "      <td>-8.327415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-3.095443</td>\n",
       "      <td>-11.162576</td>\n",
       "      <td>-3.108638</td>\n",
       "      <td>-6.810033</td>\n",
       "      <td>-3.949405</td>\n",
       "      <td>-2.309221</td>\n",
       "      <td>-0.239779</td>\n",
       "      <td>-6.866230</td>\n",
       "      <td>-6.556308</td>\n",
       "      <td>-6.815598</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0          1         2         3         4         5          6  \\\n",
       "0  -7.747974  -3.077208 -2.283786 -0.378413 -9.398509 -3.487280  -6.933934   \n",
       "1 -11.389916  -7.895541 -5.798741 -4.611126 -4.631413 -3.517211 -11.018111   \n",
       "2  -3.095443 -11.162576 -3.108638 -6.810033 -3.949405 -2.309221  -0.239779   \n",
       "\n",
       "           7         8          9  \n",
       "0 -12.972561 -2.002663 -10.997322  \n",
       "1  -9.716209 -0.054559  -8.327415  \n",
       "2  -6.866230 -6.556308  -6.815598  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(preds[:3,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Argmax gives the location of the highest value in the array, which in this case since all the values are negative is actually the smallest value.  Since we want to know the location of the largest value across the columns, we have to set the axis to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 8, 6])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[:3,:].argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_argmax = preds.argmax(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see what percentage of predictions we got right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.91549999999999998"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(preds_argmax == y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you notice this is the same accruacy that is the output of `fit`.  Accuracy is the metric being used because that is what we set it to, much higher up. And the `loss` function we set to `NLLLoss` which is the pytorch abbreviation for negative log likelihood loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plots(x_imgs[:8], titles=preds_argmax[:8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a one layer neural net, i.e., a logistic regression, and we can actually recreate the same output using scikit learn's logistic regression function.  **TODO**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### DELETE ME Integrate the original code from the notebook into my cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "310b93155e9a409290f05e68ef7fe562",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 7/782 [00:00<00:21, 35.46it/s, loss=1.23]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy   \n",
      "    0      1.213766   1.438505   0.8368    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 7/782 [00:00<00:21, 36.57it/s, loss=1.27]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    1      1.257663   1.21375    0.8687    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 6/782 [00:00<00:24, 32.17it/s, loss=1.3]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    2      1.304803   1.252223   0.8562    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 6/782 [00:00<00:23, 32.76it/s, loss=1.22]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    3      1.223197   0.916236   0.8883    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    4      1.208414   1.096059   0.8778    \n",
      "\n",
      "CPU times: user 45.4 s, sys: 30.7 s, total: 1min 16s\n",
      "Wall time: 41.4 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0960593797355891, 0.87780000000000002]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "fit(net, md, n_epochs=5, crit=loss, opt=opt, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "set_lrs(opt, 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "564018b309a9494fba10c8e89873cd0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.       0.07224  0.13539  0.97024]                          \n",
      "[ 1.       0.05924  0.13015  0.97114]                          \n",
      "[ 2.       0.04723  0.1316   0.97124]                          \n",
      "\n"
     ]
    }
   ],
   "source": [
    "fit(net, md, n_epochs=3, crit=loss, opt=opt, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea801e9b44b74358ab227a9c485a69e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.       0.27253  0.21465  0.93939]                         \n",
      "[ 1.       0.21963  0.23439  0.93481]                         \n",
      "[ 2.       0.23288  0.18333  0.94705]                         \n",
      "[ 3.       0.19822  0.1902   0.94636]                         \n",
      "[ 4.       0.205    0.27594  0.92227]                         \n",
      "\n"
     ]
    }
   ],
   "source": [
    "fit(net, md, n_epochs=5, crit=loss, opt=opt, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "set_lrs(opt, 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e3074b642a64df0894778909923b943",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.       0.07269  0.09255  0.97373]                          \n",
      "[ 1.       0.06316  0.08361  0.97572]                          \n",
      "[ 2.       0.04525  0.08077  0.97681]                          \n",
      "\n"
     ]
    }
   ],
   "source": [
    "fit(net, md, n_epochs=3, crit=loss, opt=opt, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([78400, 100, 10000, 100, 1000, 10], 89610)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = [o.numel() for o in net.parameters()]\n",
    "t, sum(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building our own Neural Network for Logistic Regression in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we used pytorch's `nn.Linear` to create a linear layer, defined as a matrix multiplication followed by an addition (these are also called `affine transformations`).  Let's try defining this ourselves, by building our own PyTorch class.  A PyTorch module is either a neural net or a layer in a nueral net (since neural nets are modular and a neural net itself can be a \"layer\" in a larger neural net)\n",
    "\n",
    "Just as Numpy has `np.matmul` for matrix multiplication (in Python 3, this is equivalent to the `@` operator), PyTorch has `torch.matmul`.  In other words `torch.matmul(x,y) == x@y`\n",
    "\n",
    "Our PyTorch class needs two things: constructor (says what the parameters are) and a forward method (how to calculate a prediction using those parameters)  The method `forward` describes how the neural net converts inputs to outputs.\n",
    "\n",
    "In PyTorch, the optimizer knows to try to optimize any attribute of type **Parameter**.  This is why above when we called the SGD optimizer (`optim.sgd`) we had to call the `.parameter()` function on our layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Aside about sub classes in python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "When creating a class, we can just add functionality to a pre-existing class by calling that class when creating your new class.  Our new class is a subclass of `nn.module` and it is inheriting the properties of the nn.module class.  There is one rule when creating a subclass, you need to include the following line in your init\n",
    "\n",
    "```python\n",
    "super().__init__()\n",
    "```\n",
    "\n",
    "This tells python to initialize the superclass (in this case the torch module) before initiating the current subclass (in this case LogReg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back to the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When initializing the random weight in the `get_weights` function, in order to prevent gradient shrinking or gradient explosion we divide the weight matrix by the size of the tensor.\n",
    "\n",
    "Since we don't know how many dimensions our data will have, we can use `*args`, which when used as a parameter in a function allows us to send a variable-length argument list.  Note that it is the use of the asterisk, \\*, that matters, the word `args` is just convention.\n",
    "\n",
    "In torch `view` is the equivalent of `reshape`, and setting a dimension to -1 tells it to infer the size of that dimension from the other dimensions\n",
    "\n",
    "```python\n",
    ">>> x = torch.randn(4, 4)\n",
    ">>> z = x.view(-1, 8)\n",
    ">>> z.size()\n",
    "    torch.Size([2, 8])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weights(*dims): return nn.Parameter(torch.randn(dims)/dims[0])\n",
    "def softmax(x): return torch.exp(x)/(torch.exp(x).sum(dim=1)[:,None])\n",
    "\n",
    "class LogReg(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1_w = get_weights(28*28, 10)  # Layer 1 weights\n",
    "        self.l1_b = get_weights(10)         # Layer 1 bias\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # equivalent to reshape in numpy \n",
    "        x = (x @ self.l1_w) + self.l1_b  # Linear Layer\n",
    "        x = torch.log(softmax(x)) # Non-linear (LogSoftmax) Layer\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create our neural net and the optimizer.  (We will use the same loss and metrics from above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net2 = LogReg().cuda()\n",
    "net2 = LogReg()\n",
    "# opt=optim.Adam(net2.parameters())\n",
    "opt=optim.SGD(net2.parameters(), 1e-1, momentum=0.9, weight_decay=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c8e80a1ca66495cbb1c9dbfd6518b96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy   \n",
      "    0      0.361127   0.305531   0.9149    \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.30553094034194944, 0.91490000000000005]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit(net2, md, n_epochs=1, crit=loss, opt=opt, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds2 = predict(net2, md.val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds2_argmax = preds2.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plots(x_imgs[:8], titles=preds2_argmax[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.91490000000000005"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(preds2_argmax == y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Not sure where this code is appropriate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dl = iter(md.trn_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "xmb,ymb = next(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 784])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4245, -0.4245, -0.4245,  ..., -0.4245, -0.4245, -0.4245],\n",
       "        [-0.4245, -0.4245, -0.4245,  ..., -0.4245, -0.4245, -0.4245],\n",
       "        [-0.4245, -0.4245, -0.4245,  ..., -0.4245, -0.4245, -0.4245],\n",
       "        ...,\n",
       "        [-0.4245, -0.4245, -0.4245,  ..., -0.4245, -0.4245, -0.4245],\n",
       "        [-0.4245, -0.4245, -0.4245,  ..., -0.4245, -0.4245, -0.4245],\n",
       "        [-0.4245, -0.4245, -0.4245,  ..., -0.4245, -0.4245, -0.4245]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(xmb.shape)\n",
    "xmb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Wrapping a tensor in `Variable` is how you let PyTorch know to keep track of the differential for this parameter as it will need to perform SGD on it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4245, -0.4245, -0.4245,  ..., -0.4245, -0.4245, -0.4245],\n",
       "        [-0.4245, -0.4245, -0.4245,  ..., -0.4245, -0.4245, -0.4245],\n",
       "        [-0.4245, -0.4245, -0.4245,  ..., -0.4245, -0.4245, -0.4245],\n",
       "        ...,\n",
       "        [-0.4245, -0.4245, -0.4245,  ..., -0.4245, -0.4245, -0.4245],\n",
       "        [-0.4245, -0.4245, -0.4245,  ..., -0.4245, -0.4245, -0.4245],\n",
       "        [-0.4245, -0.4245, -0.4245,  ..., -0.4245, -0.4245, -0.4245]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vxmb = Variable(xmb.cuda())\n",
    "vxmb = Variable(xmb)\n",
    "vxmb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.3626e-06,  1.2105e-06,  2.8107e-06,  7.3512e-06,  3.9612e-01,\n",
       "          1.7406e-04,  2.1466e-04,  3.4038e-03,  1.5325e-02,  5.8474e-01],\n",
       "        [ 2.2152e-04,  5.5954e-02,  5.2104e-04,  1.0306e-02,  6.7856e-05,\n",
       "          6.0536e-01,  4.6721e-04,  7.0908e-02,  5.2787e-02,  2.0341e-01],\n",
       "        [ 2.0823e-02,  2.3426e-05,  1.9553e-04,  5.9046e-01,  1.0586e-06,\n",
       "          1.9001e-03,  5.4244e-06,  3.8488e-01,  8.8515e-06,  1.7022e-03]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = net2(vxmb).exp(); preds[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 10])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.5847,  0.6054,  0.5905,  0.9803,  0.9996,  0.9835,  0.9921,\n",
       "          0.4827,  0.9459,  0.8103,  0.6352,  0.9750,  0.9640,  0.9042,\n",
       "          0.9222,  0.9806,  0.8631,  0.9998,  0.9300,  0.9923,  0.9897,\n",
       "          0.9733,  0.9717,  0.9304,  0.8534,  0.9967,  0.9979,  0.9072,\n",
       "          0.9595,  0.9741,  0.9957,  0.9997,  0.5420,  0.9958,  0.7623,\n",
       "          0.9162,  0.9322,  0.9749,  0.9908,  0.6102,  0.9590,  0.8889,\n",
       "          0.9988,  0.9977,  0.6241,  0.8398,  0.9533,  0.5453,  0.9243,\n",
       "          0.9640,  0.9956,  0.9500,  0.8289,  0.9947,  0.8073,  0.7368,\n",
       "          0.9902,  0.9932,  0.9977,  0.7467,  0.9724,  0.9951,  0.9824,\n",
       "          0.9456]),\n",
       " tensor([ 9,  5,  3,  3,  3,  1,  3,  4,  8,  8,  8,  9,  1,  8,\n",
       "          1,  7,  4,  0,  0,  7,  7,  3,  5,  3,  6,  6,  6,  8,\n",
       "          1,  1,  1,  0,  4,  5,  7,  9,  1,  3,  1,  2,  0,  2,\n",
       "          2,  2,  4,  3,  6,  9,  9,  8,  2,  8,  2,  6,  5,  4,\n",
       "          7,  7,  3,  8,  6,  6,  8,  7]))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.data.max(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.5847,  0.6054,  0.5905,  0.9803,  0.9996,  0.9835,  0.9921,\n",
       "          0.4827,  0.9459,  0.8103,  0.6352,  0.9750,  0.9640,  0.9042,\n",
       "          0.9222,  0.9806,  0.8631,  0.9998,  0.9300,  0.9923,  0.9897,\n",
       "          0.9733,  0.9717,  0.9304,  0.8534,  0.9967,  0.9979,  0.9072,\n",
       "          0.9595,  0.9741,  0.9957,  0.9997,  0.5420,  0.9958,  0.7623,\n",
       "          0.9162,  0.9322,  0.9749,  0.9908,  0.6102,  0.9590,  0.8889,\n",
       "          0.9988,  0.9977,  0.6241,  0.8398,  0.9533,  0.5453,  0.9243,\n",
       "          0.9640,  0.9956,  0.9500,  0.8289,  0.9947,  0.8073,  0.7368,\n",
       "          0.9902,  0.9932,  0.9977,  0.7467,  0.9724,  0.9951,  0.9824,\n",
       "          0.9456]),\n",
       " tensor([ 9,  5,  3,  3,  3,  1,  3,  4,  8,  8,  8,  9,  1,  8,\n",
       "          1,  7,  4,  0,  0,  7,  7,  3,  5,  3,  6,  6,  6,  8,\n",
       "          1,  1,  1,  0,  4,  5,  7,  9,  1,  3,  1,  2,  0,  2,\n",
       "          2,  2,  4,  3,  6,  9,  9,  8,  2,  8,  2,  6,  5,  4,\n",
       "          7,  7,  3,  8,  6,  6,  8,  7]))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.max(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 5\n",
       " 2\n",
       " 4\n",
       " 1\n",
       " 8\n",
       " 8\n",
       " 3\n",
       " 4\n",
       " 2\n",
       " 7\n",
       " 1\n",
       " 1\n",
       " 7\n",
       " 9\n",
       " 3\n",
       " 4\n",
       " 3\n",
       " 0\n",
       " 1\n",
       " 4\n",
       " 3\n",
       " 3\n",
       " 6\n",
       " 8\n",
       " 9\n",
       " 0\n",
       " 9\n",
       " 5\n",
       " 2\n",
       " 4\n",
       " 7\n",
       " 9\n",
       " 0\n",
       " 2\n",
       " 5\n",
       " 2\n",
       " 9\n",
       " 9\n",
       " 6\n",
       " 9\n",
       " 4\n",
       " 2\n",
       " 2\n",
       " 7\n",
       " 1\n",
       " 5\n",
       " 5\n",
       " 9\n",
       " 0\n",
       " 7\n",
       " 4\n",
       " 0\n",
       " 9\n",
       " 1\n",
       " 1\n",
       " 4\n",
       " 7\n",
       " 0\n",
       " 9\n",
       " 0\n",
       " 4\n",
       " 4\n",
       " 0\n",
       " 7\n",
       "[torch.cuda.LongTensor of size 64 (GPU 0)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = preds.data.max(1)[1]; preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Writing Our Own Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "As a reminder, this is what we did above to write our own logistic regression class (as a pytorch neural net):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f603dded54ac424392d35b795ad66776",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy   \n",
      "    0      0.37501    0.335794   0.9129    \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.33579445953369141, 0.91290000000000004]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Our code from above\n",
    "class LogReg(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1_w = get_weights(28*28, 10)  # Layer 1 weights\n",
    "        self.l1_b = get_weights(10)         # Layer 1 bias\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = x @ self.l1_w + self.l1_b \n",
    "        return torch.log(softmax(x))\n",
    "\n",
    "# net2 = LogReg().cuda()\n",
    "net2 = LogReg()\n",
    "# opt=optim.Adam(net2.parameters())\n",
    "opt=optim.SGD(net2.parameters(), 1e-1, momentum=0.9, weight_decay=1e-3)\n",
    "\n",
    "fit(net2, md, n_epochs=1, crit=loss, opt=opt, metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Above, we are using the fastai method `fit` to train our model.  Now we will try writing the training loop ourselves.\n",
    "\n",
    "We will use the LogReg class we created, as well as the same loss function, learning rate, and optimizer as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# net2 = LogReg().cuda()\n",
    "net2 = LogReg()\n",
    "loss=nn.NLLLoss()\n",
    "# learning_rate = 1e-3\n",
    "opt=optim.SGD(net2.parameters(), 1e-1, momentum=0.9, weight_decay=1e-3)\n",
    "# optimizer=optim.Adam(net2.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "md is the ImageClassifierData object we created above.  We want an iterable version of our training data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dl = iter(md.trn_dl) # Data loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "First, we will do a **forward pass**, which means computing the predicted `y` by passing `x` to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "xt, yt = next(dl)\n",
    "# y_pred = net2(Variable(xt).cuda())\n",
    "y_pred = net2(Variable(xt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We can check the loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.2973)\n"
     ]
    }
   ],
   "source": [
    "# l = loss(y_pred, Variable(yt).cuda())\n",
    "l = loss(y_pred, Variable(yt))\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We may also be interested in the accuracy.  We don't expect our first predictions to be very good, because the weights of our network were initialized to random values.  Our goal is to see the loss decrease (and the accuracy increase) as we train the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.125"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(to_np(y_pred).argmax(axis=1) == to_np(yt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now we will use the optimizer to calculate which direction to step in.  That is, how should we update our weights to try to decrease the loss?\n",
    "\n",
    "Pytorch has an automatic differentiation package ([autograd](http://pytorch.org/docs/master/autograd.html)) that takes derivatives for us, so we don't have to calculate the derivative ourselves!  We just call `.backward()` on our loss to calculate the direction of steepest descent (the direction to lower the loss the most)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Before the backward pass, use the optimizer object to zero all of the\n",
    "# gradients for the variables it will update (which are the learnable weights\n",
    "# of the model)\n",
    "opt.zero_grad()\n",
    "\n",
    "# Backward pass: compute gradient of the loss with respect to model parameters\n",
    "l.backward()\n",
    "\n",
    "# Calling the step function on an Optimizer makes an update to its parameters\n",
    "opt.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now, let's make another set of predictions and check if our loss is lower:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "xt, yt = next(dl)\n",
    "# y_pred = net2(Variable(xt).cuda())\n",
    "y_pred = net2(Variable(xt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.1683)\n"
     ]
    }
   ],
   "source": [
    "# l = loss(y_pred, Variable(yt).cuda())\n",
    "l = loss(y_pred, Variable(yt))\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Note that we are using **stochastic** gradient descent, so the loss is not guaranteed to be strictly better each time.  The stochasticity comes from the fact that we are using **mini-batches**; we are just using 64 images to calculate our prediction and update the weights, not the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.390625"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(to_np(y_pred).argmax(axis=1) == to_np(yt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "If we run several iterations in a loop, we should see the loss decrease and the accuracy increase with time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  tensor(2.2301) \t accuracy:  0.265625\n",
      "loss:  tensor(0.6034) \t accuracy:  0.84375\n",
      "loss:  tensor(0.6776) \t accuracy:  0.75\n",
      "loss:  tensor(0.6230) \t accuracy:  0.828125\n",
      "loss:  tensor(0.4466) \t accuracy:  0.875\n",
      "loss:  tensor(0.3114) \t accuracy:  0.921875\n",
      "loss:  tensor(0.4596) \t accuracy:  0.859375\n",
      "loss:  tensor(0.3602) \t accuracy:  0.90625\n",
      "loss:  tensor(0.4485) \t accuracy:  0.890625\n",
      "loss:  tensor(0.3115) \t accuracy:  0.890625\n"
     ]
    }
   ],
   "source": [
    "for t in range(100):\n",
    "    xt, yt = next(dl)\n",
    "#     y_pred = net2(Variable(xt).cuda())\n",
    "    y_pred = net2(Variable(xt))\n",
    "#     l = loss(y_pred, Variable(yt).cuda())\n",
    "    l = loss(y_pred, Variable(yt))\n",
    "    \n",
    "    if t % 10 == 0:\n",
    "        accuracy = np.mean(to_np(y_pred).argmax(axis=1) == to_np(yt))\n",
    "        print(\"loss: \", l.data[0], \"\\t accuracy: \", accuracy)\n",
    "\n",
    "    opt.zero_grad()\n",
    "    l.backward()\n",
    "    opt.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Put it all together in a training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def score(x, y):\n",
    "    y_pred = to_np(net2(V(x)))\n",
    "    return np.sum(y_pred.argmax(axis=1) == to_np(y))/len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.874104299363\n"
     ]
    }
   ],
   "source": [
    "# net2 = LogReg().cuda()\n",
    "net2 = LogReg()\n",
    "loss=nn.NLLLoss()\n",
    "learning_rate = 1e-2\n",
    "optimizer=optim.SGD(net2.parameters(), lr=learning_rate)\n",
    "trn_batches = md.trn_ds.n // md.bs + int(md.trn_ds.n % md.bs > 0)\n",
    "val_batches = md.val_ds.n // md.bs + int(md.val_ds.n % md.bs > 0)\n",
    "\n",
    "for epoch in range(1):\n",
    "    losses=[]\n",
    "    dl = iter(md.trn_dl)\n",
    "#     for t in range(len(dl)):\n",
    "    for t in range(trn_batches):\n",
    "        # Forward pass: compute predicted y and loss by passing x to the model.\n",
    "        xt, yt = next(dl)\n",
    "        y_pred = net2(V(xt))\n",
    "        l = loss(y_pred, V(yt))\n",
    "        losses.append(l)\n",
    "\n",
    "        # Before the backward pass, use the optimizer object to zero all of the\n",
    "        # gradients for the variables it will update (which are the learnable weights of the model)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Backward pass: compute gradient of the loss with respect to model parameters\n",
    "        l.backward()\n",
    "\n",
    "        # Calling the step function on an Optimizer makes an update to its parameters\n",
    "        optimizer.step()\n",
    "    \n",
    "    val_dl = iter(md.val_dl)\n",
    "    val_scores = [score(*next(val_dl)) for i in range(val_batches)]\n",
    "#     val_scores = [score(*next(val_dl)) for i in range(len(val_dl))]\n",
    "    print(np.mean(val_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nearly all of deep learning is powered by one very important algorithm: **stochastic gradient descent (SGD)**. SGD can be seeing as an approximation of **gradient descent (GD)**. In GD you have to run through all the samples in your training set to do a single itaration. In SGD you use only a subset of training samples to do the update for a parameter in a particular iteration. The subset used in each iteration is called a batch or minibatch.\n",
    "\n",
    "Now, instead of using the optimizer, we will do the optimization ourselves!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.871218152866\n",
      "0.885250796178\n",
      "0.893113057325\n"
     ]
    }
   ],
   "source": [
    "# net2 = LogReg().cuda()\n",
    "net2 = LogReg()\n",
    "loss_fn=nn.NLLLoss()\n",
    "lr = 1e-2\n",
    "w,b = net2.l1_w,net2.l1_b\n",
    "trn_batches = md.trn_ds.n // md.bs + int(md.trn_ds.n % md.bs > 0)\n",
    "val_batches = md.val_ds.n // md.bs + int(md.val_ds.n % md.bs > 0)\n",
    "\n",
    "for epoch in range(3):\n",
    "    losses=[]\n",
    "    dl = iter(md.trn_dl)\n",
    "#     for t in range(len(dl)):\n",
    "    for t in range(trn_batches):\n",
    "        xt, yt = next(dl)\n",
    "        y_pred = net2(V(xt))\n",
    "#         l = loss(y_pred, Variable(yt).cuda())\n",
    "        l = loss(y_pred, Variable(yt))\n",
    "        losses.append(loss)\n",
    "\n",
    "        # Backward pass: compute gradient of the loss with respect to model parameters\n",
    "        l.backward()\n",
    "        w.data -= w.grad.data * lr\n",
    "        b.data -= b.grad.data * lr\n",
    "        \n",
    "        w.grad.data.zero_() # suffix underscore just means do the operation in place instead of creating new variable\n",
    "        b.grad.data.zero_()   \n",
    "\n",
    "    val_dl = iter(md.val_dl)\n",
    "    val_scores = [score(*next(val_dl)) for i in range(val_batches)]\n",
    "#     val_scores = [score(*next(val_dl)) for i in range(len(val_dl))]\n",
    "    print(np.mean(val_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Addenda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the number of parameters per layer in my neural network by calling the `.parameters()` method on my neural network.  This creates a generator object which will go through each layer,  If I then iterate through the generator I can extract the number of elements in each layer using `.numel()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([7840, 10], 7850)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = [o.numel() for o in net.parameters()]\n",
    "t, sum(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2 Regularization and weight decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that we calculate our parameters by minimizing a loss function, which has the general form of :\n",
    "\n",
    "$$ \\ell = \\sum\\limits_{i=1}^n\\left(Y_i - \\sum\\limits_{j=1}^p X_{ij} w_j\\right)^2 $$\n",
    "\n",
    "One way to reduce the overfitting of our model is to penalize all non-zero weights.  We can do this by adding a term to our loss function:\n",
    "\n",
    "$$ \\ell = \\sum\\limits_{i=1}^n\\left(Y_i - \\sum\\limits_{j=1}^p X_{ij} w_j\\right)^2 + \\ell_{reg}(w)$$\n",
    "\n",
    "We don't want to penalize weights with a value zero, as that is the same as not having any parameter, just the degree to which the weights differ from zero.  Two standard ways of doing this are L1 regularization, also known as lasso regression (least absolute shrinkage and selection operator), and L2 regularization.  L1 regularization uses the absolute value of the weights and L2 uses the square of the weights.  In both cases you don't want to add the entire value of the weights to the loss function, just some small percentage, on the order of $10^{-4}$ to $10^{-6}$, which we can call $\\alpha$.\n",
    "\n",
    "$$ \\ell_{L1} = \\sum\\limits_{i=1}^n\\left(Y_i - \\sum\\limits_{j=1}^p X_{ij} w_j\\right)^2 + \\alpha\\sum\\limits_{j=1}^p\\left|w_j\\right| $$\n",
    "\n",
    "$$ \\ell_{L2} = \\sum\\limits_{i=1}^n\\left(Y_i - \\sum\\limits_{j=1}^p X_{ij} w_j\\right)^2 + \\alpha\\sum\\limits_{j=1}^p w_j^2 $$\n",
    "\n",
    "When working with neural networks we are often concerned with the gradient of the loss, which for L2 is easy to calculate\n",
    "\n",
    "$$\\Delta_{L2} = 2\\alpha w $$\n",
    "\n",
    "In neural networks, this L2 gradient is referred to as the weight decay.  \n",
    "\n",
    "Let's build a lsightly deeper simple neural network with and without weight decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(\n",
    "    nn.Linear(28*28, 100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100, 100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100, 10),\n",
    "    nn.LogSoftmax()\n",
    ")\n",
    "md = ImageClassifierData.from_arrays(PATH, (x,y), (x_valid, y_valid))\n",
    "loss=nn.NLLLoss()\n",
    "metrics=[accuracy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt=optim.SGD(net.parameters(), 1e-1, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44ffeb46314b470eb6f591219a6933f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 4/782 [00:00<00:36, 21.48it/s, loss=0.294]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy   \n",
      "    0      0.301842   0.24314    0.9398    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 4/782 [00:00<00:37, 20.89it/s, loss=0.244]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    1      0.246357   0.264417   0.9415    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    2      0.24186    0.250231   0.9451    \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.25023104345500469, 0.94510000000000005]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit(net, md, n_epochs=3, crit=loss, opt=opt, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(\n",
    "    nn.Linear(28*28, 100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100, 100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100, 10),\n",
    "    nn.LogSoftmax()\n",
    ")\n",
    "opt=optim.SGD(net.parameters(), 1e-1, momentum=0.9, weight_decay=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eceb14122ed24fa0aea2909528fd84c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 4/782 [00:00<00:36, 21.27it/s, loss=0.274]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy   \n",
      "    0      0.273734   0.244744   0.9298    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 4/782 [00:00<00:37, 20.94it/s, loss=0.231]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    1      0.232462   0.275376   0.9212    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    2      0.231492   0.191098   0.9475    \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.19109784487709403, 0.94750000000000001]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit(net, md, n_epochs=3, crit=loss, opt=opt, metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might expect the training loss to be worse with weight decay, but what might be happening here is that weight decay is making the loss function easier to optimize on a per epoch level, resulting in a lower training loss.  But in the end you would expect the training loss with weight decay to be worse (larger value) than the training loss without weight decay.\n",
    "\n",
    "However, weight decay should improve the overall loss on the validation set, which it does in this case.  This is because the purpose of the weight decay is to decrease overfitting, which should make the model more generalizable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "512px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
